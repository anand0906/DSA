# Hash Collisions & Resolution Techniques

> Complete guide to understanding and handling hash collisions

---

## ğŸ“‹ Table of Contents

- [What is a Hash Collision?](#what-is-a-hash-collision)
- [Collision Resolution Techniques](#collision-resolution-techniques)
- [Open Chaining (Separate Chaining)](#open-chaining-separate-chaining)
- [Open Addressing](#open-addressing)
- [Comparison & Best Practices](#comparison--best-practices)

---

## âš¡ What is a Hash Collision?

### Definition

A **hash collision** occurs when two different keys produce the same hash value and map to the same index in the hash table.

### Why Collisions Happen

```
Hash Function: Maps infinite possible keys â†’ Finite hash table slots

Pigeonhole Principle: If you have more items than slots,
                      at least two items must share a slot!
```

### Simple Example

**Hash Function:** `h(key) = len(key) % 10`

```
Input: "cat"
Length: 3
Hash: 3 % 10 = 3  â† Index 3

Input: "dog"  
Length: 3
Hash: 3 % 10 = 3  â† Index 3 (COLLISION!)
```

**Visual:**
```
Hash Table:
Index:  0    1    2   [3]   4    5    6
       â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
       â”‚    â”‚    â”‚    â”‚ ?? â”‚    â”‚    â”‚    â”‚
       â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
                        â†‘
                   Both "cat" and "dog"
                   want this slot!
```

### Real-World Example

**Employee ID System:**
```
Hash Function: h(id) = id % 100

Employee 10523 â†’ 10523 % 100 = 23
Employee 20623 â†’ 20623 % 100 = 23  â† Collision!

Both IDs hash to slot 23
```

### Collision Visualization

```
Keys: 15, 25, 35, 20
Hash Function: h(k) = k % 10

15 % 10 = 5  â”€â”€â”
25 % 10 = 5  â”€â”€â”¼â”€â”€â†’ All hash to index 5!
35 % 10 = 5  â”€â”€â”˜
20 % 10 = 0  â”€â”€â”€â”€â†’ Index 0

Index:  0    1    2    3    4    5    6
       â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
       â”‚ 20 â”‚    â”‚    â”‚    â”‚    â”‚ ?? â”‚    â”‚
       â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
                                    â†‘
                            3 keys collide!
```

---

## ğŸ”§ Collision Resolution Techniques

### Two Main Approaches

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Collision Resolution Methods      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  1. Open Chaining                   â”‚
â”‚     (Separate Chaining)             â”‚
â”‚     â€¢ Store multiple items per slot â”‚
â”‚     â€¢ Use linked lists/trees        â”‚
â”‚                                     â”‚
â”‚  2. Open Addressing                 â”‚
â”‚     â€¢ Find another empty slot       â”‚
â”‚     â€¢ Probe sequence:               â”‚
â”‚       - Linear Probing              â”‚
â”‚       - Quadratic Probing           â”‚
â”‚       - Double Hashing              â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Open Chaining (Separate Chaining)

### Concept

Each hash table slot contains a **linked list** (or other data structure) to store all elements that hash to that index.

### How It Works

```
Step 1: Calculate hash index
Step 2: If collision occurs, add to linked list at that index
Step 3: Search by traversing the linked list
Step 4: Delete by finding in list and removing
```

### Visual Representation

```
Hash Table with Chaining:

Index    Linked List
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  0   â†’  [20] â†’ NULL
  
  1   â†’  NULL
  
  2   â†’  NULL
  
  3   â†’  NULL
  
  4   â†’  NULL
  
  5   â†’  [15] â†’ [25] â†’ [35] â†’ NULL
         â†‘      â†‘      â†‘
      First  Second  Third
      
  6   â†’  NULL
  
  7   â†’  NULL
  
  8   â†’  NULL
  
  9   â†’  NULL
```

### Step-by-Step Example

**Hash Function:** `h(k) = k % 10`
**Keys to Insert:** 15, 25, 35, 20

#### Step 1: Insert 15
```
h(15) = 15 % 10 = 5

Index 5: Empty
Action: Create linked list with [15]

  5   â†’  [15] â†’ NULL
```

#### Step 2: Insert 25
```
h(25) = 25 % 10 = 5

Index 5: Contains [15]
Action: Add 25 to the list (COLLISION!)

  5   â†’  [15] â†’ [25] â†’ NULL
```

#### Step 3: Insert 35
```
h(35) = 35 % 10 = 5

Index 5: Contains [15] â†’ [25]
Action: Add 35 to the list (COLLISION!)

  5   â†’  [15] â†’ [25] â†’ [35] â†’ NULL
```

#### Step 4: Insert 20
```
h(20) = 20 % 10 = 0

Index 0: Empty
Action: Create linked list with [20]

  0   â†’  [20] â†’ NULL
```

### Operations

#### Search Operation
```
Search for key 25:

Step 1: Calculate h(25) = 5
Step 2: Go to index 5
Step 3: Traverse linked list:
        [15] â‰  25, continue
        [25] = 25, FOUND! âœ“

Time Complexity: O(1 + Î±)
where Î± = load factor (n/m)
```

#### Delete Operation
```
Delete key 25:

Step 1: Calculate h(25) = 5
Step 2: Go to index 5
Step 3: Traverse and remove:
        [15] â†’ [25] â†’ [35]
               â†“ Remove
        [15] â†’ [35]

Before: [15] â†’ [25] â†’ [35] â†’ NULL
After:  [15] â†’ [35] â†’ NULL
```

### Python Implementation

```python
class MyHash:
    def __init__(self, b):
        self.BUCKET = b
        self.table = [[] for x in range(b)]
    
    def insert(self, x):
        i = x % self.BUCKET
        self.table[i].append(x)
    
    def remove(self, x):
        i = x % self.BUCKET
        if x in self.table[i]:
            self.table[i].remove(x)
    
    def search(self, x):
        i = x % self.BUCKET
        return x in self.table[i]

# Example usage
h = MyHash(7)
h.insert(70)
h.insert(71)
h.insert(9)
h.insert(56)
h.insert(72)
print(h.search(56))  # True
h.remove(56)
print(h.search(56))  # False
```

### Advanced Data Structures for Chaining

#### 1. Linked Lists (Standard)
```
Time Complexity:
â€¢ Insert: O(1)
â€¢ Search: O(n)
â€¢ Delete: O(n)

Memory: Low overhead
```

#### 2. Binary Search Trees (BST/AVL/Red-Black)
```
Time Complexity:
â€¢ Insert: O(log n)
â€¢ Search: O(log n)
â€¢ Delete: O(log n)

Memory: Moderate overhead
Best for: Many collisions
```

#### 3. Python Lists
```
Time Complexity:
â€¢ Insert: O(1) average
â€¢ Search: O(n)
â€¢ Delete: O(n)

Memory: Low overhead
Best for: Simple implementation
```

### Comparison of Chain Data Structures

| Data Structure | Insert | Search | Delete | Memory | Complexity |
|---------------|--------|--------|--------|---------|------------|
| Linked List | O(1) | O(n) | O(n) | Low | Simple |
| AVL Tree | O(log n) | O(log n) | O(log n) | Moderate | Complex |
| Red-Black Tree | O(log n) | O(log n) | O(log n) | Moderate | Complex |
| Python List | O(1) | O(n) | O(n) | Low | Simple |

### Advantages of Open Chaining

âœ… **Simple to Implement**
- Straightforward logic
- Easy to understand

âœ… **No Table Overflow**
- Can handle any number of collisions
- Lists grow dynamically

âœ… **Easy Deletion**
- Remove from linked list
- No special handling needed

âœ… **Less Sensitive to Hash Function**
- Poor hash functions still work reasonably
- Graceful degradation

### Disadvantages of Open Chaining

âŒ **Extra Memory for Pointers**
- Each node needs pointer storage
- Overhead can be significant

âŒ **Cache Performance**
- Linked lists are cache-unfriendly
- Random memory access

âŒ **Degraded Performance with High Load**
- Long chains slow down operations
- Needs good hash function

---

## ğŸ¯ Open Addressing

### Concept

All elements are stored **within the hash table itself**. When a collision occurs, we **probe** for the next available slot using a systematic sequence.

### General Formula

```
h(k, i) = (h'(k) + f(i)) % table_size

where:
  k = key
  i = probe attempt (0, 1, 2, 3, ...)
  h'(k) = primary hash function
  f(i) = probing function
```

### Three Main Techniques

```
1. Linear Probing:     f(i) = i
2. Quadratic Probing:  f(i) = iÂ²
3. Double Hashing:     f(i) = i Ã— hâ‚‚(k)
```

---

## ğŸ“ Linear Probing

### Formula

```
h(k, i) = (h(k) + i) % table_size

If collision at h(k), try:
â€¢ h(k) + 0
â€¢ h(k) + 1
â€¢ h(k) + 2
â€¢ h(k) + 3
... until empty slot found
```

### Visual Example

**Keys:** 50, 700, 76, 85, 92, 73, 101
**Hash Function:** `h(k) = k % 10`

#### Step-by-Step Insertion

**1. Insert 50:**
```
h(50) = 50 % 10 = 0
Slot 0 empty â†’ Insert

Index:  0    1    2    3    4    5    6    7    8    9
       [50] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]
```

**2. Insert 700:**
```
h(700) = 700 % 10 = 0
Slot 0 occupied â†’ Try next
Slot 1 empty â†’ Insert

Index:  0    1    2    3    4    5    6    7    8    9
       [50][700][ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]
```

**3. Insert 76:**
```
h(76) = 76 % 10 = 6
Slot 6 empty â†’ Insert

Index:  0    1    2    3    4    5    6    7    8    9
       [50][700][ ] [ ] [ ] [ ] [76] [ ] [ ] [ ]
```

**4. Insert 85:**
```
h(85) = 85 % 10 = 5
Slot 5 empty â†’ Insert

Index:  0    1    2    3    4    5    6    7    8    9
       [50][700][ ] [ ] [ ] [85][76] [ ] [ ] [ ]
```

**5. Insert 92:**
```
h(92) = 92 % 10 = 2
Slot 2 empty â†’ Insert

Index:  0    1    2    3    4    5    6    7    8    9
       [50][700][92][ ] [ ] [85][76] [ ] [ ] [ ]
```

**6. Insert 73:**
```
h(73) = 73 % 10 = 3
Slot 3 empty â†’ Insert

Index:  0    1    2    3    4    5    6    7    8    9
       [50][700][92][73][ ] [85][76] [ ] [ ] [ ]
```

**7. Insert 101:**
```
h(101) = 101 % 10 = 1
Slot 1 occupied â†’ Try next (linear probing)

Attempt 0: (1 + 0) % 10 = 1  [Occupied by 700]
Attempt 1: (1 + 1) % 10 = 2  [Occupied by 92]
Attempt 2: (1 + 2) % 10 = 3  [Occupied by 73]
Attempt 3: (1 + 3) % 10 = 4  [Empty] â†’ Insert

Index:  0    1    2    3    4    5    6    7    8    9
       [50][700][92][73][101][85][76][ ] [ ] [ ]
```

### Primary Clustering Problem

```
When consecutive slots fill up:

Bad Distribution:
[50][700][92][73][101][85][76][ ][ ][ ]
 â—†â”€â—†â”€â”€â—†â”€â”€â—†â”€â”€â”€â—†â”€â”€â”€â—†â”€â”€â—†  (cluster)

Problem: New insertions likely to add to cluster!

If we insert key with hash 4, 5, 6, or 7:
â€¢ Must probe through entire cluster
â€¢ Cluster grows longer
â€¢ Performance degrades
```

### Python Implementation

```python
class LinearProbingHashTable:
    def __init__(self, size):
        self.size = size
        self.table = [None] * size
    
    def _hash(self, key):
        return key % self.size
    
    def insert(self, key):
        index = self._hash(key)
        i = 0
        while self.table[(index + i) % self.size] is not None:
            i += 1
        self.table[(index + i) % self.size] = key
    
    def search(self, key):
        index = self._hash(key)
        i = 0
        while self.table[(index + i) % self.size] is not None:
            if self.table[(index + i) % self.size] == key:
                return True
            i += 1
        return False
```

---

## ğŸ“ Quadratic Probing

### Formula

```
h(k, i) = (h(k) + iÂ²) % table_size

If collision at h(k), try:
â€¢ h(k) + 0Â² = h(k) + 0
â€¢ h(k) + 1Â² = h(k) + 1
â€¢ h(k) + 2Â² = h(k) + 4
â€¢ h(k) + 3Â² = h(k) + 9
â€¢ h(k) + 4Â² = h(k) + 16
```

### Visual Example

**Keys:** 50, 700, 76, 85, 92, 73, 101
**Hash Function:** `h(k) = k % 10`

#### Step-by-Step with Quadratic Probing

**Insert 101:**
```
h(101) = 101 % 10 = 1
Slot 1 occupied â†’ Quadratic probing

i=0: (1 + 0Â²) % 10 = 1  [Occupied]
i=1: (1 + 1Â²) % 10 = 2  [Occupied]
i=2: (1 + 2Â²) % 10 = 5  [Occupied]
i=3: (1 + 3Â²) % 10 = 10 % 10 = 0  [Occupied]
i=4: (1 + 4Â²) % 10 = 17 % 10 = 7  [Empty] â†’ Insert

Final Table:
Index:  0    1    2    3    4    5    6    7    8    9
       [50][700][92][73][ ] [85][76][101][ ] [ ]
```

### Probing Sequence Comparison

```
Linear vs Quadratic Probing:

Starting at index 1:

Linear:     1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7 â†’ 8 â†’ 9
Quadratic:  1 â†’ 2 â†’ 5 â†’ 0 â†’ 7 â†’ 6 â†’ 7 â†’ 0 â†’ 5
            (+0)(+1)(+4)(+9)(+16)

Quadratic jumps around more!
Reduces primary clustering.
```

### Secondary Clustering

```
Problem: Keys with same h(k) follow same probe sequence

Example:
Key A: h(A) = 3 â†’ tries 3, 4, 7, 2, 9, ...
Key B: h(B) = 3 â†’ tries 3, 4, 7, 2, 9, ... (same!)

Secondary clustering: Keys hashing to same slot
                     probe identically
```

### Python Implementation

```python
class QuadraticProbingHashTable:
    def __init__(self, size):
        self.size = size
        self.table = [None] * size
    
    def _hash(self, key):
        return key % self.size
    
    def insert(self, key):
        index = self._hash(key)
        i = 0
        while self.table[(index + i * i) % self.size] is not None:
            i += 1
        self.table[(index + i * i) % self.size] = key
    
    def search(self, key):
        index = self._hash(key)
        i = 0
        while self.table[(index + i * i) % self.size] is not None:
            if self.table[(index + i * i) % self.size] == key:
                return True
            i += 1
        return False
```

---

## ğŸ”€ Double Hashing

### Concept

Uses **two hash functions** to determine probe sequence. Most effective open addressing method!

### Formula

```
h(k, i) = (hâ‚(k) + i Ã— hâ‚‚(k)) % table_size

where:
  hâ‚(k) = primary hash (initial position)
  hâ‚‚(k) = secondary hash (step size)
  
Common choices:
  hâ‚(k) = k % table_size
  hâ‚‚(k) = 1 + (k % (table_size - 1))
```

### Why Two Hash Functions?

```
Different keys â†’ Different probe sequences!

Key A: hâ‚(A) = 5, hâ‚‚(A) = 3
       Probes: 5 â†’ 8 â†’ 1 â†’ 4 â†’ 7 â†’ ...

Key B: hâ‚(B) = 5, hâ‚‚(B) = 7
       Probes: 5 â†’ 2 â†’ 9 â†’ 6 â†’ 3 â†’ ...

Same starting point, DIFFERENT paths!
Eliminates secondary clustering.
```

### Detailed Example

**Keys:** 50, 700, 76, 85, 92, 73, 101
**Table Size:** 10
**hâ‚(k) = k % 10**
**hâ‚‚(k) = 1 + (k % 9)**

#### Insert 50
```
hâ‚(50) = 50 % 10 = 0
hâ‚‚(50) = 1 + (50 % 9) = 1 + 5 = 6

Slot 0 empty â†’ Insert at 0

  0    1    2    3    4    5    6    7    8    9
[50] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]
```

#### Insert 700
```
hâ‚(700) = 700 % 10 = 0 (collision!)
hâ‚‚(700) = 1 + (700 % 9) = 1 + 7 = 8

Probe sequence:
i=0: (0 + 0Ã—8) % 10 = 0  [Occupied]
i=1: (0 + 1Ã—8) % 10 = 8  [Empty] â†’ Insert

  0    1    2    3    4    5    6    7    8    9
[50] [ ] [ ] [ ] [ ] [ ] [ ] [ ][700][ ]
```

#### Insert 76
```
hâ‚(76) = 76 % 10 = 6
hâ‚‚(76) = 1 + (76 % 9) = 1 + 4 = 5

Slot 6 empty â†’ Insert at 6

  0    1    2    3    4    5    6    7    8    9
[50] [ ] [ ] [ ] [ ] [ ] [76] [ ][700][ ]
```

#### Insert 85
```
hâ‚(85) = 85 % 10 = 5
hâ‚‚(85) = 1 + (85 % 9) = 1 + 4 = 5

Slot 5 empty â†’ Insert at 5

  0    1    2    3    4    5    6    7    8    9
[50] [ ] [ ] [ ] [ ] [85][76] [ ][700][ ]
```

#### Insert 92
```
hâ‚(92) = 92 % 10 = 2
hâ‚‚(92) = 1 + (92 % 9) = 1 + 2 = 3

Slot 2 empty â†’ Insert at 2

  0    1    2    3    4    5    6    7    8    9
[50] [ ] [92][ ] [ ] [85][76] [ ][700][ ]
```

#### Insert 73
```
hâ‚(73) = 73 % 10 = 3
hâ‚‚(73) = 1 + (73 % 9) = 1 + 1 = 2

Slot 3 empty â†’ Insert at 3

  0    1    2    3    4    5    6    7    8    9
[50] [ ] [92][73][ ] [85][76] [ ][700][ ]
```

#### Insert 101
```
hâ‚(101) = 101 % 10 = 1
hâ‚‚(101) = 1 + (101 % 9) = 1 + 2 = 3

Slot 1 empty â†’ Insert at 1

  0    1    2    3    4    5    6    7    8    9
[50][101][92][73][ ] [85][76] [ ][700][ ]
```

### Probe Sequence Visualization

```
Key 101: hâ‚ = 1, hâ‚‚ = 3

Probe path if collisions occurred:
i=0: (1 + 0Ã—3) % 10 = 1
i=1: (1 + 1Ã—3) % 10 = 4
i=2: (1 + 2Ã—3) % 10 = 7
i=3: (1 + 3Ã—3) % 10 = 0
i=4: (1 + 4Ã—3) % 10 = 3
i=5: (1 + 5Ã—3) % 10 = 6
i=6: (1 + 6Ã—3) % 10 = 9
i=7: (1 + 7Ã—3) % 10 = 2
i=8: (1 + 8Ã—3) % 10 = 5
i=9: (1 + 9Ã—3) % 10 = 8

Visits all slots! (when table_size and step are coprime)
```

### Python Implementation

```python
class DoubleHashingHashTable:
    def __init__(self, size):
        self.size = size
        self.table = [None] * size
    
    def _hash1(self, key):
        return key % self.size
    
    def _hash2(self, key):
        return 1 + (key % (self.size - 1))
    
    def insert(self, key):
        index = self._hash1(key)
        step_size = self._hash2(key)
        i = 0
        while self.table[(index + i * step_size) % self.size] is not None:
            i += 1
        self.table[(index + i * step_size) % self.size] = key
    
    def search(self, key):
        index = self._hash1(key)
        step_size = self._hash2(key)
        i = 0
        while self.table[(index + i * step_size) % self.size] is not None:
            if self.table[(index + i * step_size) % self.size] == key:
                return True
            i += 1
        return False
```

---

## ğŸ“Š Comparison & Best Practices

### Method Comparison Table

| Method | Time (Avg) | Time (Worst) | Memory | Clustering | Complexity |
|--------|-----------|--------------|---------|------------|------------|
| **Chaining** | O(1+Î±) | O(n) | High | None | Simple |
| **Linear Probing** | O(1/(1-Î±)) | O(n) | Low | Primary | Simple |
| **Quadratic Probing** | O(1/(1-Î±)) | O(n) | Low | Secondary | Medium |
| **Double Hashing** | O(1/(1-Î±)) | O(n) | Low | Minimal | Medium |

*Î± = load factor (n/m)*

### Load Factor Impact

```
Load Factor (Î±) = Number of keys / Table size

Examples:
â€¢ 10 keys, 20 slots â†’ Î± = 0.5 (50% full)
â€¢ 15 keys, 20 slots â†’ Î± = 0.75 (75% full)
â€¢ 18 keys, 20 slots â†’ Î± = 0.9 (90% full)

Performance vs Load Factor:

Î± = 0.5  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    (Good)
Î± = 0.75 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 (OK)
Î± = 0.9  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           (Poor)
Î± = 1.0  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (Full!)

Recommendation:
â€¢ Chaining: Î± < 1.0 (can exceed)
â€¢ Open Addressing: Î± < 0.7 (resize before this)
```

### When to Use Each Method

#### Use Chaining When:
âœ… Load factor may exceed 0.7
âœ… Deletions are frequent
âœ… Memory isn't critically constrained
âœ… Want simple implementation
âœ… Unpredictable number of keys

#### Use Linear Probing When:
âœ… Load factor stays low (< 0.5)
âœ… Cache performance matters
âœ… Memory is limited
âœ… Simple implementation needed
âœ… Few deletions

#### Use Quadratic Probing When:
âœ… Want better distribution than linear
âœ… Load factor moderate (< 0.7)
âœ… Avoiding primary clustering matters
âœ… Table size is prime

#### Use Double Hashing When:
âœ… Best performance needed
âœ… Load factor can be higher (< 0.7)
âœ… Willing to implement two hash functions
âœ… Minimizing clustering is critical
âœ… Distribution quality matters most

### Practical Guidelines

**1. Hash Function Quality**
```
Good hash function is CRITICAL for open addressing!

Bad:  h(k) = k % 10     (only uses last digit)
Good: h(k) = (k Ã— 2654435761) >> (32 - logâ‚‚(m))
```

**2. Table Size**
```
âœ… Use prime numbers for table size
âœ… Especially important for:
   â€¢ Division method
   â€¢ Quadratic probing
   â€¢ Double hashing

Examples: 7, 11, 13, 17, 23, 31, 53, 97, 199, 401
```

**3. Resizing Strategy**
```
When to resize:
â€¢ Chaining: Î± > 1.0
â€¢ Open Addressing: Î± > 0.7

How to resize:
1. Create new table (2Ã— size, next prime)
2. Rehash all elements
3. Replace old table

Cost: O(n) but amortized O(1)
```

**4. Deletion Handling**
```
Open Addressing Problem:
Deleting creates gaps â†’ breaks search!

Solution: Use "tombstone" markers
â€¢ Mark slot as "deleted" (not empty)
â€¢ Search continues through tombstones
â€¢ Insert can reuse tombstone slots

[50][DEL][92][73] â†’ Search continues past DEL
```

### Performance Summary

**Best Overall:** Double Hashing
- Minimal clustering
- Good distribution
- Handles high load factors

**Simplest:** Chaining with Linked Lists
- Easy to implement
- Works with any load factor
- Predictable behavior

**Most Cache-Friendly:** Linear Probing
- Sequential memory access
- Best for small tables
- Low load factors only

**Balanced Choice:** Chaining with Python Lists
- Simple implementation
- Good performance
- Flexible and practical

---

## ğŸ“ Summary

### Key Takeaways

1. **Collisions are Inevitable**
   - Finite table, infinite keys
   - Good hash functions minimize but don't eliminate

2. **Two Main Resolution Strategies**
   - **Chaining:** Store multiple items per slot
   - **Open Addressing:** Find another empty slot

3. **Trade-offs Exist**
   - Memory vs Speed
   - Simplicity vs Performance
   - Guaranteed vs Average complexity

4. **Choice Depends on Context**
   - Load factor expectations
   - Memory constraints
   - Performance requirements
   - Implementation complexity tolerance

5. **Quality Matters**
   - Hash function quality is critical
   - Table size affects performance
   - Load factor management is essential

### Final Recommendations

**For Learning:** Start with chaining using Python lists
**For Production:** Consider double hashing or chaining with BSTs
**For Embedded Systems:** Linear probing with low load factor
**For General Use:** Chaining with linked lists (most implementations use this)

Understanding collision resolution is fundamental to implementing efficient hash-based data structures like hash tables, dictionaries, and sets!
